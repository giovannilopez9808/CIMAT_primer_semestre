\section*{Problema 2}
\textbf{Implement the following algorithms: Bisection, Newton, and Secant methods for optimization in 1D.}

\subsection*{Método de bisección}

El método de bisección supone que la función $f$ es unimodal, esto quiere decir que existe un único valor crítico. La función $f$ también debe de cumplir que es continuamente diferenciable. Dada una x en el intervalo $[a_0,b_0]$, el método obtiene una secuencia de intervalos tal que:

\begin{equation*}
    [a_0,b_0]\supset [a_1,b_1]\supset [a_2,b_2] \supset \dots \supset [a_k,b_k]
\end{equation*}

En cada intervalo, el valor crítico estara contenido. El algoritmo para obtener el valor mínimo de una función es el siguiente:
\begin{lstlisting}[language=python]
    input: df(x), [a,b], tolerancia
    output: x, approximacion valor minimo
    x=(a+b)/2
    while |b-a| > tolerancia:
        if (df(x) > 0):
            b=x
        else:
            a=x
        x=(a+b)/2
    return x
\end{lstlisting}

En el caso que se quiera aplicar el algoritmo para obtener el valor máximo de una función se deberá realizar una modificación en la linea 4, ya que si no, el algoritmo arrojara como resultado alguno de los extremos del intervalo. La modificación sería la siguiente:

\begin{lstlisting}[language=python]
    input: df(x), [a,b], tolerancia
    output: x, approximacion valor maximo
    x=(a+b)/2
    while |b-a| > tolerancia:
        if (df(x) < 0):
            b=x
        else:
            a=x
        x=(a+b)/2
    return x
\end{lstlisting}

La implementación del algoritmo se encuentra en la carpeta \textcolor{title}{Problema\_3a} y \textcolor{title}{Problema\_3b} en el archivo \textcolor{citecolor}{bisection.h}.

\subsection*{Método de Newton}

El método de Newton supone que tenemos una $f$, la cual es de clase $C^2$, lo cual quiere decir que la primer y segunda derivada de $f$ son continuas. El algoritmo del descendiente es el siguiente:
Dado un valor pequeño de h, donde $h>0$, entonces el siguiente valor de x es:

\begin{equation}
    x_{k+1} = x_k -hf'(x_k)
    \label{eq:descent_algorithm}
\end{equation}

Con la ecuación \ref{eq:descent_algorithm} se obtienen los siguientes casos:
\begin{itemize}
    \item Si $f'(x_0)>0$ entonces $x_{k+1}<x_0$, ya que existe $\delta>0$ para $f'(x)>0$ para todo $|x-x_0|<\delta$. Por lo tanto $f$ es creciente en el interalo.
    \item Si $f'(x_0)<0$ entonces $x_{k+1}>x_0$, ya que existe $\delta>0$ para $f'(x)<0$ para todo $|x-x_0|<\delta$. Por lo tanto $f$ es decreciente en el interalo.
\end{itemize}

Podemos aproximar la función $f$ usando una serie de Taylor hasta segundo orden. Entnces:

\begin{equation*}
    f(x) \approx q(x) \overset{def}{=} f(x_0)+ f'(x_0)(x-x_0) + \frac{1}{2} f''(x_0)(x-x_0)^2
\end{equation*}

Por lo que al querer minimizar $f$, el método de Newton genera la siguiente secuencia

\begin{equation*}
    x_{k+1} = arg \underset{x}{min} q(x)
\end{equation*}

Calculando la primer derivada de $q(x)$, obtenemos lo siguiente:

\begin{align*}
    q'(x) & = f'(x_k)+ f''(x_k) (x-x_k)
\end{align*}

En donde al encontrar sus valores críticos, obtenemos la secuencia para el método de Newton (ecuación \ref{eq:secuancia_newton}).

\begin{equation}
    x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
    \label{eq:secuancia_newton}
\end{equation}

El algoritmo del método de Newton es el siguiente:

\begin{lstlisting}[language=python]
    input: df, ddf, x0, tolerancia
    output: x, aproxximacion de sus valores criticos
    while |df| > tolerancia:
        x = x- df/dff
    return x
\end{lstlisting}

La implementación del método de Newton se encuentra en las carpetas \textcolor{citecolor}{Problema\_3a} y \textcolor{citecolor}{Problema\_3b} en el archivo \textcolor{title}{newton.h}.

\subsection*{Método de secante}

El método de la secante es una alternativa al método de Newton. Supongamos que no se tiene la manera de obtener la segunda derivada de $f$. Entonces podemos aproximar este valor con la ecuación \ref{eq:approximation_second_derivate}.

\begin{equation}
    f''(x_k) = \frac{f'(x_k)-f'(x_{k-1})}{x_k-x_{k-1}}
    \label{eq:approximation_second_derivate}
\end{equation}

Sustituyendo la ecuación \ref{eq:approximation_second_derivate} en \ref{eq:secuancia_newton}, obtenemos lo siguiente:

\begin{align}
    x_{k+1} & = x_k - \frac{x_k-x_{k-1}}{f'(x_k)-f'(x_{k-1})} f'(x_k) \nonumber                  \\
    x_{k+1} & = \frac{f'(x_k)x_{k-1}-f'(x_k)x_k}{f'(x_k)-f'(x_{k-1})} \label{eq:sequence_secant}
\end{align}

Por lo tanto, el algoritmo del método de la secante es el siguiente:

\begin{lstlisting}[language=python]
    input: df, x0, x1, tolerancia
    output: x1, aproxximacion de sus valores criticos
    while |df| > tolerancia:
        x2 = x1 - df(x1)(x1-x0)/(df(x1)-df(x0))
        x0, x1 = x1, x2
    return x1
\end{lstlisting}

La implementación del método de Newton se encuentra en las carpetas \textcolor{citecolor}{Problema\_3a} y \textcolor{citecolor}{Problema\_3b} en el archivo \textcolor{title}{secant.h}.